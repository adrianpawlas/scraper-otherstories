name: Midnight Fashion Scraper

on:
  schedule:
    # Run at midnight UTC every day (adjust timezone as needed)
    - cron: '0 0 * * *'
  workflow_dispatch: # Allow manual triggering

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget gnupg
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable

    - name: Set up Chrome for headless mode
      run: |
        echo "CHROME_BIN=/usr/bin/google-chrome-stable" >> $GITHUB_ENV
        echo "HEADLESS=true" >> $GITHUB_ENV

    - name: Create necessary directories
      run: |
        mkdir -p logs cache/embeddings

    - name: Run scraper
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        USER_AGENT: ${{ secrets.USER_AGENT || 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36' }}
        HEADLESS: true
        LOG_LEVEL: INFO
        PYTHONPATH: ${{ github.workspace }}:${{ env.PYTHONPATH }}
      run: |
        python -m src.main --mode full

    - name: Upload logs on failure
      if: failure()
      uses: actions/upload-artifact@v4
      with:
        name: scraper-logs-${{ github.run_id }}
        path: logs/

    - name: Commit and push results (optional)
      if: success()
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add logs/
        git commit -m "Automated scrape results - $(date)" || echo "No changes to commit"
        git push || echo "Push failed (this is normal if no changes)"

    - name: Send notification on failure
      if: failure()
      run: |
        echo "Scraper failed - check logs for details"
        # Add webhook notifications here if needed
